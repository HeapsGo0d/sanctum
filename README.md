# ğŸ”’ Sanctum - Privacy-Focused Ollama + Open WebUI for RunPod

**Minimal Â· Privacy-First Â· RunPod-Native**

Sanctum is a privacy-focused RunPod template for running Ollama + Open WebUI with telemetry blocking. Built for simplicity and security.

## âœ¨ Features

- **ğŸ”’ Privacy-First**: Telemetry blocking via /etc/hosts (22 analytics domains)
- **âš¡ Fast Startup**: Two services, no unnecessary operations
- **ğŸ¯ Minimal**: Clean architecture, essential functionality only
- **ğŸ’¾ Persistent Storage**: Models and data survive pod restarts
- **ğŸ® GPU Support**: Automatic NVIDIA GPU detection and configuration
- **ğŸ”§ RunPod-Native**: Designed specifically for RunPod deployment

## ğŸš€ Quick Start (RunPod)

### 1. Create Template

Use the template generator script:

```bash
./template.sh
```

Or for automatic API deployment:

```bash
export RUNPOD_API_KEY="your_runpod_api_key"
./template.sh --deploy
```

### 2. Deploy Pod

1. Go to RunPod Templates
2. Upload `sanctum_template.json` (or use API-deployed template)
3. Deploy pod with GPU (RTX 4090, A100, etc.)
4. Wait for startup (~30 seconds)

### 3. Access Open WebUI

Once running, access Open WebUI at:
```
http://[your-pod-id]-8080.proxy.runpod.net
```

**Note**: RunPod provides SSH access automatically. No additional configuration needed.

## ğŸ§ª Local Testing

Test locally with Docker Compose:

```bash
# Build and run
docker-compose up

# Access Open WebUI
open http://localhost:8080
```

Test privacy mode:
```bash
# Shell into container
docker exec -it sanctum bash

# Check blocked domains
grep "0.0.0.0" /etc/hosts
```

## ğŸ“‹ Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_HOST` | `0.0.0.0` | Ollama server bind address |
| `OLLAMA_MODELS` | `/workspace/models` | Ollama models directory |
| `OLLAMA_NUM_PARALLEL` | `2` | Number of parallel requests |
| `OLLAMA_BASE_URL` | `http://127.0.0.1:11434` | Ollama API URL for Open WebUI |
| `DATA_DIR` | `/workspace/data` | Open WebUI data directory |
| `WEBUI_AUTH` | `False` | Enable Open WebUI authentication |
| `WEBUI_PORT` | `8080` | Open WebUI port |
| `PRIVACY_MODE` | `enabled` | Enable telemetry blocking (`enabled`/`disabled`) |

## ğŸ”’ Privacy Features

### Telemetry Blocking

When `PRIVACY_MODE=enabled` (default):

Blocks common analytics and tracking domains:
- Google Analytics, Tag Manager
- Segment, Amplitude, Mixpanel
- Sentry, PostHog, Hotjar
- AI/ML tracking (OpenAI, Anthropic, HuggingFace)

View blocked domains:
```bash
grep "0.0.0.0" /etc/hosts
```

### Disable Privacy Mode

To disable privacy protections:
```bash
PRIVACY_MODE=disabled
```

Or edit in RunPod template environment variables.

## ğŸ’¾ Storage

### Persistent Data

- `/workspace/models` - Ollama models (survives restarts)
- `/workspace/data` - Open WebUI data (survives restarts)

### Volume Configuration

Set volume size in template or RunPod UI:
- **0GB**: Ephemeral (models redownload on restart)
- **20GB+**: Persistent (recommended for production)

## ğŸ”§ Usage

### Download Models

When you first open Open WebUI you'll see "No models available" â€” this is expected. Ollama is running but has no models downloaded yet.

**Via the UI:**
1. Click the model selector dropdown at the top
2. Type a model name and click **Search Ollama.com** â€” or â€”
3. Go to **Admin Panel** â†’ **Settings** â†’ **Models**, enter a model name in the pull field, and click the download button

**Via the terminal** (faster for large models):
```bash
# Shell into the container
docker exec -it <container_id> bash

# Pull any model by its Ollama Hub name
ollama pull llama3.2:3b
ollama pull mistral:7b
ollama pull qwen2.5:7b

# List downloaded models
ollama list
```

Browse available models at https://ollama.com/library. Use the full `name:tag` format when pulling (e.g. `llama3.2:3b`, not just `llama3.2`).

Models are stored in `/workspace/models` and persist across restarts.

### Via SSH (RunPod Host-Level)

SSH into your RunPod instance and use Ollama CLI:

```bash
# Pull a model
ollama pull llama2

# List models
ollama list

# Run a model
ollama run llama2

# Check service status
curl http://localhost:11434/api/tags
curl http://localhost:8080
```

## ğŸ› Troubleshooting

### Services Not Starting

Check logs:
```bash
# Ollama logs
tail -f /tmp/ollama.log

# WebUI logs
tail -f /tmp/webui.log
```

### GPU Not Detected

Verify GPU is available:
```bash
nvidia-smi
```

### Check Privacy Status

View blocked telemetry domains:
```bash
grep "0.0.0.0" /etc/hosts
```

### Health Check Failures

Manually check service health:
```bash
/scripts/health-check.sh
```

## ğŸ“ Project Structure

```
sanctum/
â”œâ”€â”€ Dockerfile                              # Container definition
â”œâ”€â”€ README.md                               # This file
â”œâ”€â”€ docker-compose.yml                      # Local testing
â”œâ”€â”€ template.sh                             # RunPod template generator
â”œâ”€â”€ sanctum_template.json                  # Generated template
â”œâ”€â”€ .github/workflows/build-and-push.yml   # Auto build/push
â””â”€â”€ scripts/
    â”œâ”€â”€ startup.sh                          # Main entrypoint
    â”œâ”€â”€ health-check.sh                    # Service verification
    â””â”€â”€ privacy/
        â””â”€â”€ setup-blocklist.sh              # /etc/hosts blocking
```

## ğŸ—ï¸ Development

### Build Locally

```bash
docker build -t sanctum:dev .
```

### Run Locally

```bash
docker run -d \
  --privileged \
  -p 8080:8080 \
  -v $(pwd)/test-workspace:/workspace \
  -e PRIVACY_MODE=enabled \
  sanctum:dev
```

### Push to Docker Hub

Configure GitHub secrets:
- `DOCKER_USERNAME`
- `DOCKER_PASSWORD`

Push to main branch triggers automatic build/push.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Test thoroughly (local + RunPod)
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ” Security Notes

- **Privacy Scope**: Blocks known telemetry/analytics domains via `/etc/hosts`
- **Limitations**: Does not provide full network isolation (open internet access remains)
- **Production Use**: Consider additional network policies (firewalls, VPNs) for stricter isolation
- **Authentication**: Default `WEBUI_AUTH=False` - enable for public deployments
- **Graceful Degradation**: `/etc/hosts` blocking skipped on read-only filesystems

## ğŸ™ Acknowledgments

Built with:
- [Ollama](https://ollama.com) - Run large language models locally
- [Open WebUI](https://github.com/open-webui/open-webui) - ChatGPT-like interface
- [RunPod](https://runpod.io) - GPU cloud platform

Inspired by privacy-focused projects in the self-hosted AI community.

---

**ğŸ”’ Sanctum - Private AI, Your Way**
